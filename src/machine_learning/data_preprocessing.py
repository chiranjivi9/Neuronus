import json 
import os
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# 1. Data Preprocessing
    # The first step is to prepare the data generated by generate_entries.py. 
    # This involves loading the data, handling missing values, and 
    # normalizing the features to ensure they are in a 
    # consistent range for the machine learning model.

def preprocess_data():
    # 1.1 Loading the Data
        # Get the absolute path to the directory containing the current script
    script_dir = os.path.dirname(os.path.abspath(__file__))

    # Construct the absolute path to 'generated_analytics.json' file
    json_file_path = os.path.join(script_dir, 'helpers', 'generated_analytics.json')

    # Load the analytics data from the JSON file
    try:
        with open(json_file_path, 'r') as f:
            data = json.load(f)
        print("Data loaded successfully!")
    except FileNotFoundError:
        print(f"File not found: {json_file_path}")
        
    # Convert the data to a Pandas DataFrame
    df = pd.DataFrame(data)

    print('1.1 Convert data to Pandas DataFrame')
    print(df.head())
    
    # 1.2 Handling Missing Values
        # In real-world datasets, some values might be missing or incomplete. 
        # In our case, a key might not have a TTL or might not have been accessed often. 
        # It’s important to handle these missing values before proceeding.
        # 
        # You can either fill missing values with default values (like 0) or 
        # drop rows with missing data. Here, we'll fill missing TTL values with 0.


    # Check for missing values in the DataFrame
    print('1.2 Check for missing values in the DataFrame')
    print(df.isnull().sum())

    # Fill missing values in the 'ttl' column with 0. Alternatively, we can use median or drop rows
    df['ttl'] = df['ttl'].fillna(0)
    
    
    # 1.3 Normalizing Features
        # In machine learning, it’s common to normalize or scale features so that
        # they are on a similar range. This helps the model interpret the data more consistently,
        # especially when different features are measured on
        # very different scales (e.g., access_count vs. ttl).
    
    # We'll use MinMaxScaler to normalize access_count, size, and ttl to a range between 0 and 1.

    # Why Normalization?
        # If one feature has much larger values (e.g., size) compared to another (e.g., access_count), 
        # the model might give more importance to the larger feature. Normalization ensures that each feature contributes equally.  

    #  Initialize the scaler
    scaler = MinMaxScaler()

    # Normalize 'access_count', 'size', 'ttl' columns
    df[['access_count', 'size', 'ttl']] = scaler.fit_transform(df[['access_count', 'size', 'ttl']])

    # Inspect the normalize data
    print('1.3 Inspect the normalize data')
    print(df.head())
    
    return df


preprocess_data()